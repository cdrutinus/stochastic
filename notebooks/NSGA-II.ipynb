{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Optimization\n",
    "\n",
    "## 3.3 NSGA-II\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://supaerodatascience.github.io/stochastic/\">https://supaerodatascience.github.io/stochastic/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore NSGA-II, we'll use the [PyMOO](https://pymoo.org/algorithms/nsga2.html) library and a Multi-Objective Travelling Salesman Problem. For the different objectives, we'll construct random distance matrices, but we could imagine, for example, that one objective is travel time between two points and a second objective is travel cost. We want to minimize both objectives, choosing a route from the Pareto front of quick and low-cost travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the Multi-Objective Travelling Salesman Problem using the `Problem` class from `pymoo`. We'll define it to take any number of cities and objectives, returning a fitness of the total distance from each objective for the given individual. We specify a constraint on the order to ensure we're visiting each city once, but we'll also define the same operators as in the GA notebook to make sure all individuals meet this constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.model.problem import Problem\n",
    "\n",
    "class MOTSP(Problem):\n",
    "    \n",
    "    def __init__(self, n_cities, n_obj):\n",
    "        # lower bound\n",
    "        xl = np.zeros(n_cities)\n",
    "        # upper bound\n",
    "        xu = (n_cities-1) * np.ones(n_cities)\n",
    "        \n",
    "        self.n_cities = n_cities\n",
    "        self.distances = []\n",
    "        for i in range(n_obj):\n",
    "            # random symmetric matrix\n",
    "            d = np.random.rand(n_cities, n_cities)\n",
    "            d = np.tril(d) + np.tril(d, -1).T\n",
    "            d[np.diag_indices(n_cities)] = 0\n",
    "            self.distances.append(d)\n",
    "        \n",
    "        super().__init__(n_var=n_cities, n_obj=n_obj, n_constr=1,\n",
    "                        xl=xl, xu=xu, elementwise_evaluation=True)\n",
    "        \n",
    "    def total_distance(self, x, d):\n",
    "        t = 0\n",
    "        for i in range(1, len(x)):\n",
    "            t += self.distances[d][x[i-1], x[i]]\n",
    "        return t\n",
    "    \n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        # fitness based on each distance matrix\n",
    "        fits = np.zeros(len(self.distances))\n",
    "        for i in range(len(self.distances)):\n",
    "            fits[i] = self.total_distance(x, i)\n",
    "        \n",
    "        # constraints return negative if met\n",
    "        c = -np.sum(np.arange(self.n_cities) != np.sort(x))\n",
    "        \n",
    "        # return by modifying out\n",
    "        out[\"F\"] = np.column_stack(fits)\n",
    "        out[\"G\"] = np.array([c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define NSGA2, its hyperparameters and operators. We'll use the same operators as before. Note that `pymoo` allows for generating fewer offspring than the initial population size and that it can check for duplicates. We'll use this second feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.algorithms.nsga2 import NSGA2\n",
    "from pymoo.factory import get_sampling, get_crossover, get_mutation\n",
    "from pymoo.factory import get_termination\n",
    "\n",
    "algorithm = NSGA2(\n",
    "    pop_size=100,\n",
    "    n_offsprings=100,\n",
    "    sampling=get_sampling(\"perm_random\"),\n",
    "    crossover=get_crossover(\"perm_erx\", prob=0.9),\n",
    "    mutation=get_mutation(\"perm_inv\"),\n",
    "    eliminate_duplicates=True\n",
    ")\n",
    "termination = get_termination(\"n_gen\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's define a 50-city problem with 2 objectives. As mentioned, these could be total travel time and total travel cost. We want to minimize both objectives. Note the use of `seed=1` in the call to `minimize`: this optimization is deterministic (we can run it multiple times and get the same result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = MOTSP(50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.optimize import minimize\n",
    "\n",
    "res = minimize(problem,\n",
    "               algorithm,\n",
    "               termination,\n",
    "               seed=1,\n",
    "               save_history=True,\n",
    "               verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `res` object returned from the search contains the results, including the final Pareto front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = Scatter(tight_layout=True)\n",
    "plot.add(res.F, s=10)\n",
    "plot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the convergence of each objective. We'll plot the fitness of the first individual in the Pareto front over evolution. Note that this isn't the best value for each objective independently, but rather the objective values of a single individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_evals = np.array([e.evaluator.n_eval for e in res.history])\n",
    "opt = np.array([e.opt[0].F for e in res.history])\n",
    "\n",
    "plt.title(\"Convergence\")\n",
    "plt.plot(n_evals, opt)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of dimensions grows, visualizing the convergence across all objectives becomes difficult. One recently proposed metric known as the \"running metric\" evaluates non-dominated sets from each generation relative to previously recorded sets. This gives an idea of how much the Pareto front has moved in any generation. For visualizing this running metric, each ND set is normalized by the final ND set after a certain number of generations. The difference between intervals then gives an idea if the algorithm is converging.\n",
    "\n",
    "\n",
    "Blank, Julian, and Kalyanmoy Deb. \"A running performance metric and termination criterion for evaluating evolutionary multi-and many-objective optimization algorithms.\" Proc. IEEE World Congr. Comput. Intell.(WCCI). 2020.\n",
    "\n",
    "https://www.egr.msu.edu/~kdeb/papers/c2020003.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.util.running_metric import RunningMetric\n",
    "\n",
    "running = RunningMetric(delta_gen=20,\n",
    "                        n_plots=5,\n",
    "                        only_if_n_plots=True,\n",
    "                        key_press=False,\n",
    "                        do_show=True)\n",
    "\n",
    "for algorithm in res.history:\n",
    "    running.notify(algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise 1</h3>\n",
    "\n",
    "Increase the number of objectives and observe the convergence of NSGA-II. Roughly, how much does each objective change convergence speed?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise 2</h3>\n",
    "\n",
    "Visualizing populations in many-objective optimization is also difficult. Look at the [visualization](http://pymoo.org/visualization/index.html) options in pymoo and try to visualize populations with more than 5 objectives. What plot makes most sense to you?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
