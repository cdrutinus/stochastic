{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Optimization\n",
    "\n",
    "## 2.1 Evolutionary Strategies\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://supaerodatascience.github.io/stochastic/\">https://supaerodatascience.github.io/stochastic/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The $(1+\\lambda)$ ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Last class, we saw random optimization, otherwise known as the $(1+1)$ Evolutionary Strategy. As a reminder, here is the algorithm: \n",
    "\n",
    "    Initialize x randomly in ‚Ñù\n",
    "    while not terminate\n",
    "        x' = x + ùëÅ(0, 1)\n",
    "        if f(x‚Ä≤) < f(x)\n",
    "            x = x'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this algorithm, at each step, we randomly sample from a Normal distribution to get one new point near the current point. What if, instead of sampling one point, we sampled multiple points? We'll look at an example with the Himmelblau function from last class. As a reminder, we'll first visualize the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def himmelblau(x, y):\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker, cm\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.arange(-5, 5, 0.1)\n",
    "Y = np.arange(-5, 5, 0.1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = himmelblau(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral',\n",
    "                 norm=colors.Normalize(vmin=Z.min(), vmax=Z.max()), alpha=0.4)\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 10))\n",
    "ax = plt.axes(projection='3d')    \n",
    "ax.plot_surface(X, Y, Z, cmap='Spectral', rcount=100,\n",
    "                ccount=100, norm=colors.Normalize(vmin=Z.min(), vmax=Z.max()),\n",
    "                shade=False, antialiased=True, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll start with a random point $x$ as we've done before. We'll then use a Normal distribution to draw points around $x$, this time generating 20 points around $x$ instead of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(2) * 4 - 2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x_t = np.array([x + np.random.normal(size=(2,)) for i in range(20)])\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral',\n",
    "                 norm=colors.Normalize(vmin=Z.min(), vmax=Z.max()), alpha=0.4)\n",
    "plt.scatter(x_t[:, 0], x_t[:, 1], c='b')\n",
    "plt.scatter(x[0], x[1], c='r')\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Having this population of points will enable us to explore the space around $x$ before moving on to the next step, which can better inform movement in the search space. However, these points might overlap or cover spaces that have already been explored. Generating too many points could slow down search, requiring more evaluations of the objective function. In many applications, that is costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Performing random optimization but sampling more than 1 point leads to the $(1+\\lambda)$ Evolutionary Strategy. \n",
    "\n",
    "    Initialize x randomly in ‚Ñù\n",
    "    while not terminate\n",
    "        x_p = x\n",
    "        for i in [1,Œª]\n",
    "            x_i = x_p + ùëÅ(0, 1)\n",
    "            if f(x_i) < f(x)\n",
    "                x = x_i\n",
    "        x_p = x\n",
    "    return x_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the scope of evolutionary algorithms, we'll refer to $x$ as a parent and all $x'$ points as offspring. Each point can also be referred to as an individual in a population, and $f(x)$ is called the fitness of the individual. Each iteration of the algorithm is called a generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $(\\mu/\\rho,\\lambda)$ or $(\\mu/\\rho+\\lambda)$ notation signifies the configuration of parents and offspring. In this notation, $\\mu$ is the number of parents, $\\rho$ is the number of parents involved in creating the offspring, and $\\lambda$ is the number of offspring. $(\\mu/\\rho+\\lambda)$ means the parents can be included in the next population, whereas $(\\mu/\\rho,\\lambda)$ means the parents are not included. $(1+\\lambda)$ therefore means that 1 parent is involved in creating a population of $\\lambda$ offspring and can be included in the next generation (ie, if $f(x) < f(x') \\forall x'$, $x$ does not change)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the next section, we'll see CMA-ES, a $(\\mu/\\mu_I,\\lambda)$ algorithm where a transformation of $\\mu$, $\\mu_I$, will inform the next population but it will not be included in the next population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise 1</h3>\n",
    "    Implement the $(1+\\lambda)$ evolutionary strategy and test it on the Himmelblau function. Study the impact of the $\\lambda$ parameter by modifying it and re-running the optimization. What is the best $\\lambda$ value for this problem?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/4_1plusES.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximating the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the $(1+\\lambda)$ ES, we move from the best point in a population to the best point in the next randomly sampled population. However, with the population information, we can do better than that; we can move in the direction of the gradient approximated by the fitness values of the population. This can also better use the full population information, combining all fitness information instead of selecting the best fitness value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x_t = np.array([x + np.random.normal(size=(2,)) for i in range(10)])\n",
    "fits = himmelblau(x_t[:, 0], x_t[:, 1])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
    "plt.quiver(x_t[:, 0], x_t[:, 1], x_t[:, 0]-x[0], x_t[:, 1]-x[1],\n",
    "           fits, scale=fits/100, scale_units='xy', cmap='Spectral', norm=norm)\n",
    "plt.scatter(x[0], x[1], c='k')\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First we normalize all fitness values by the average $\\mu(f(x))$ and standard deviation $\\sigma(f(x))$ of the population. This is to make the movement based on the relative fitness in the population as opposed to the absolute fitness in the search space.\n",
    "\n",
    "$A = \\frac{f(x) - \\mu(f(x))}{\\sigma(f(x))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll then define a vector at each point, $x_i - x$. Note that since $x_i = x + N(0, 1)$, we could store just this original random sampled point from $N(0, 1)$ as our vector.\n",
    "\n",
    "$N_i = x_i - x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We multiply $A$ by all individual vectors which scales the vector magnitude by the relative fitness. Finally, we sum all vectors and divide by $\\lambda$ to get the weighted average.\n",
    "\n",
    "$\\frac{1}{\\lambda}\\sum_i A_i N_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to the dot product and is used as our gradient approximation. Since we're minimizing, we'll use the negative value to do gradient descent.\n",
    "\n",
    "$\\nabla f \\approx -\\frac{A \\cdot N}{\\lambda}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A = (fits - np.mean(fits)) / np.std(fits)\n",
    "N = x_t - x\n",
    "G = -np.dot(A, N) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
    "plt.quiver(x_t[:, 0], x_t[:, 1], x_t[:, 0]-x[0], x_t[:, 1]-x[1],\n",
    "           fits, scale=fits/100, scale_units='xy', cmap='Spectral', norm=norm)\n",
    "plt.quiver(x[0], x[1], G[0], G[1])\n",
    "plt.scatter(x[0], x[1], c='k')\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have our approximate gradient direction, but how far should we move in it? We'll define a learning rate variable $\\alpha$ as the fixed magnitude of movement for now, so our update of $x$ is then\n",
    "\n",
    "$x = x - \\alpha\\frac{A \\cdot N}{\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's put this all together as an Evolutionary Strategy\n",
    "\n",
    "    Initialize x randomly in ‚Ñù\n",
    "    x_best = x\n",
    "    while not terminate\n",
    "        for i in [1,Œª]\n",
    "            N_i = ùëÅ(0, 1)\n",
    "            F_i = f(x + N_i)\n",
    "            if F_i < f(x_best)\n",
    "                x_best = x + N_i\n",
    "        A = (F‚àíùúá(F))/ùúé(F)\n",
    "        x = x - ùõº(A‚ãÖN)/ùúÜ\n",
    "    return x_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Discussion</h3>\n",
    "    \n",
    "What is this Evolutionary Strategy in $(+,)$ notation? Is it a $(1+\\lambda)$ ES? Why or why not? Discuss in your group.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise 2</h3>\n",
    "    \n",
    "Implement this evolutionary strategy and test it on the Himmelblau function. Study the impact of the $\\alpha$ parameter by modifying it and re-running the optimization. How does this ES compare to the $(1+\\lambda)$ ES?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/5__es.py"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
